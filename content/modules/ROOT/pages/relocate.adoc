:hub_openshift_api_server_url: {hub_cluster_openshift_api_server_url} 
:hub_openshift_cluster_console_url: {hub_cluster_openshift_cluster_console_url} 
:hub_openshift_cluster_admin_username: {hub_cluster_openshift_cluster_admin_username} 
:hub_openshift_cluster_admin_password: {hub_cluster_openshift_cluster_admin_password} 
:hub_gitea_console_url: {hub_cluster_gitea_console_url} 
:hub_gitea_admin_username: {hub_cluster_gitea_admin_username} 
:hub_gitea_admin_password: {hub_cluster_gitea_admin_password} 
:hub_bastion_public_hostname: {hub_cluster_bastion_public_hostname} 
:hub_bastion_ssh_password: {hub_cluster_bastion_ssh_password} 
:hub_bastion_ssh_user_name: {hub_cluster_bastion_ssh_user_name} 
:hub_ssh_command: {hub_cluster_ssh_command} 

:primary_openshift_api_server_url: {primary_cluster_openshift_api_server_url} 
:primary_openshift_cluster_console_url: {primary_cluster_openshift_cluster_console_url} 
:primary_openshift_cluster_admin_username: {primary_cluster_openshift_cluster_admin_username} 
:primary_openshift_cluster_admin_password: {primary_cluster_openshift_cluster_admin_password} 
:primary_bastion_public_hostname: {primary_cluster_bastion_public_hostname} 
:primary_bastion_ssh_password: {primary_cluster_bastion_ssh_password} 
:primary_bastion_ssh_user_name: {primary_cluster_bastion_ssh_user_name} 
:primary_ssh_command: {primary_cluster_ssh_command} 

:secondary_openshift_api_server_url: {secondary_cluster_openshift_api_server_url} 
:secondary_openshift_cluster_console_url: {secondary_cluster_openshift_cluster_console_url} 
:secondary_openshift_cluster_admin_username: {secondary_cluster_openshift_cluster_admin_username} 
:secondary_openshift_cluster_admin_password: {secondary_cluster_openshift_cluster_admin_password} 
:secondary_bastion_public_hostname: {secondary_cluster_bastion_public_hostname} 
:secondary_bastion_ssh_user_name: {secondary_cluster_bastion_ssh_user_name} 
:secondary_bastion_ssh_password: {secondary_cluster_bastion_ssh_password} 
:secondary_ssh_command: {secondary_cluster_ssh_command} 

=== Relocate Application To Primary

[arabic]
. You will now attempt to relocate/failback the application to the primary cluster.  Before we do that, we want to ensure that any changes made on the secondary cluster are preserved when relocating back to the primary cluster. In the terminal session, login to the secondary cluster link:{secondary_openshift_api_server_url}[Secondary Cluster OpenShift API Link] with your credentials username: {secondary_openshift_cluster_admin_username} and password: {secondary_openshift_cluster_admin_password}.
+
[source,role="execute",subs="attributes"]
----
oc login -u {secondary_openshift_cluster_admin_username} -p {secondary_openshift_cluster_admin_password} {secondary_openshift_api_server_url}
----
+
. You are again going to make an update to the price of the Quarkus T-shirt.  Update the price by running the below commands:
+
[source,role="execute",subs="attributes"]
----
POD=$(oc get pods -n globex | grep catalog-database | awk '{print $1}')
oc exec -it $POD -n globex -- psql --dbname catalog --command "update catalog set price = 20 where name = 'Quarkus T-shirt';"
----
+
. Confirm if the price has updated to by switching back to the Globex application on the secondary cluster and refresh the page.  Select the last page (6) and notice that the price of the Quarkust T-shirt is now *$20.00*
+
image:secondary-price-updated.png[secondary-price-updated.png]
+
. Go back to the Hub Cluster RHACM console {hub_openshift_cluster_console_url} with the user / password and select *Applications* and filter on *Subscription*.  Click the 3 dots at the end of the *globex* application and select *Relocate application*.
+
image:rhacm-relocate-menu.png[rhacm-relocate-menu.png]
+
. Provide the following details on the popup:
.. *Select policy*: drsync5m
.. *Target cluster*: primary
.. *Select subscriptions group*: globex-subscription-1
+
image:rhacm-relocate-application.png[rhacm-relocate-application.png]
+
NOTE: The subscription group may not be in a ready state to select.  Wait a few minutes and try again.
.. Click *Initiate*
. You will notice that the pods on the secondary cluster in the *globex* namespace will start terminating as the app starts relocating back to the primary cluster.
+
image:secondary-pods-terminated.png[secondary-pods-terminated.png]
+
Review the status of the pvc and the cephblockpool on secondary. The health may be temporary in warning state and then goes back to healthy state. You may also see `states:map[replaying:2]`:
+
[source,role="execute",subs="attributes"]
----
oc login -u {secondary_openshift_cluster_admin_username} -p {secondary_openshift_cluster_admin_password} {secondary_openshift_api_server_url}
----
+
[source,role="execute",subs="attributes"]
----
oc get cephblockpool ocs-storagecluster-cephblockpool -n openshift-storage -o jsonpath='{.status.mirroringStatus.summary}{"\n"}'
----
+
.Example output.
----
map[daemon_health:OK health:OK image_health:OK states:map[replaying:2]]
----
+
[source,role="execute",subs="attributes"]
----
oc get pvc -n globex
----
+
.Example output.
----
NAME                 STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS                  AGE
catalog-database     Terminating    pvc-faae1a1e-4476-4052-9461-bb7e495b5903   5Gi        RWO            ocs-storagecluster-ceph-rbd   45m
inventory-database   Terminating    pvc-fda1a2a3-cb5a-495d-b9c0-47ab95bba2c7   5Gi        RWO            ocs-storagecluster-ceph-rbd   45m
----
+
. Switch to the *primary* cluster console and click *Workloads->Pods* on the left menu.  Ensure your selected project is *globex*.  Notice that the application pods have been recreated in this namespace. You will see the pods in primary once all the pvc's complete termination in secondary cluster.
+
image:primary-pods-created.png[primary-pods-created.png]
+
. Next, click *Networking->Routes* on the left menu and then click on the *Location* link of the *globex-ui* route.
+
image:primary-globex-route.png[primary-globex-route.png]
+
. Once you've clicked the link you will be redirected to globex online store front end.  Select *Cool Stuff Store* at the top of the page.
+
image:primary-globex-landing.png[primary-globex-landing.png]
+
. Select the last page (6) and notice that the price of the Quarkust T-shirt is still *$20.00*.
+
image:secondary-price-updated.png[secondary-price-updated.png]
+
Review the status of the pvc (terminating) and the cephblockpool on primary. The health may be temporary in warning state and then goes back to healthy state:
+
[source,role="execute",subs="attributes"]
----
oc login -u {primary_openshift_cluster_admin_username} -p {primary_openshift_cluster_admin_password} {primary_openshift_api_server_url}
----
+
[source,role="execute",subs="attributes"]
----
oc get cephblockpool ocs-storagecluster-cephblockpool -n openshift-storage -o jsonpath='{.status.mirroringStatus.summary}{"\n"}'
----
+
.Example output.
----
map[daemon_health:OK health:OK image_health:OK states:map[replaying:2]]
----
+
[source,role="execute",subs="attributes"]
----
oc get pvc -n globex
----
+
.Example output.
----
NAME                 STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS                  AGE
catalog-database     Bound    pvc-faae1a1e-4476-4052-9461-bb7e495b5903   5Gi        RWO            ocs-storagecluster-ceph-rbd   18m
inventory-database   Bound    pvc-fda1a2a3-cb5a-495d-b9c0-47ab95bba2c7   5Gi        RWO            ocs-storagecluster-ceph-rbd   18m
----
