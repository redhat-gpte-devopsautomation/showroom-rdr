:hub_openshift_api_server_url: %hub_openshift_api_server_url%
:hub_openshift_cluster_console_url: %hub_openshift_cluster_console_url%
:hub_openshift_cluster_admin_username: %hub_openshift_cluster_admin_username%
:hub_openshift_cluster_admin_password: %hub_openshift_cluster_admin_password%
:hub_gitea_console_url: %hub_gitea_console_url%
:hub_gitea_admin_username: %hub_gitea_admin_username%
:hub_gitea_admin_password: %hub_gitea_admin_password%
:hub_bastion_public_hostname: %hub_bastion_public_hostname%
:hub_bastion_ssh_password: %hub_bastion_ssh_password%
:hub_bastion_ssh_user_name: %hub_bastion_ssh_user_name%
:hub_ssh_command: %hub_ssh_command%

:primary_openshift_api_server_url: %primary_openshift_api_server_url%
:primary_openshift_cluster_console_url: %primary_openshift_cluster_console_url%
:primary_openshift_cluster_admin_username: %primary_openshift_cluster_admin_username%
:primary_openshift_cluster_admin_password: %primary_openshift_cluster_admin_password%
:primary_bastion_public_hostname: %primary_bastion_public_hostname%
:primary_bastion_ssh_password: %primary_bastion_ssh_password%
:primary_bastion_ssh_user_name: %primary_bastion_ssh_user_name%
:primary_ssh_command: %primary_ssh_command%

:secondary_openshift_api_server_url: %secondary_openshift_api_server_url%
:secondary_openshift_cluster_console_url: %secondary_openshift_cluster_console_url%
:secondary_openshift_cluster_admin_username: %secondary_openshift_cluster_admin_username%
:secondary_openshift_cluster_admin_password: %secondary_openshift_cluster_admin_password%
:secondary_bastion_public_hostname: %secondary_bastion_public_hostname%
:secondary_bastion_ssh_user_name: %secondary_bastion_ssh_user_name%
:secondary_bastion_ssh_password: %secondary_bastion_ssh_password%
:secondary_ssh_command: %secondary_ssh_command%

=== Failover Application To Secondary

[arabic]
. In the terminal session, login to the primary cluster api link:{primary_openshift_cluster_console_url}[Primary Cluster OpenShift Cluster API link] with your credentials username: {primary_openshift_cluster_admin_username} and password: {primary_openshift_cluster_admin_password}.
+
[source,role="execute"]
----
oc login -u %primary_openshift_cluster_admin_username% -p %primary_openshift_cluster_admin_password% %primary_openshift_api_server_url%
----
+
. You are now going to make an update to the price of one of the globex store items.  This data is stored in a postgres database attached to a PVC which we will preserve during the failover.  Update the price by running the below commands:
+
[source,role="execute"]
----
POD=$(oc get pods -n globex | grep catalog-database | awk '{print $1}')
oc exec -it $POD -n globex -- psql --dbname catalog --command "update catalog set price = 15 where name = 'Quarkus T-shirt';"
----
+
. Switch back to the Globex application and refresh the page.  Select the last page (6) and notice that the price of the Quarkust T-shirt is now *$15.00*
+
image:images/primary-globex-item-updated.png[images/primary-globex-item-updated.png]
+
. Return to the RHACM Hub console and then click *Data Services->Data policies* from the left menu.
+
image:images/rhacm-data-policies.png[images/rhacm-data-policies.png]
+
. Click on the 3 dots on the *drsync5m* disaster recovery policy and select *Apply DRPolicy*.
image:images/rhacm-apply-dr-policy.png[imagesrhacm-apply-dr-policy.png]
+
. On the *Apply DRPolicy* popup, select the *globex* application and under *PVC label* enter *app=globex*.  These options allow the globex application to be included in the DR policy and preserves any PVC's with a label matching *app=globex*, in our case the PVC associated with the postgres database containing our prices.  Click *Apply*.
+
image:images/rhacm-save-dr-policy.png[images/rhacm-save-dr-policy.png]
+
. In the terminal session, login to the secondary cluster {secondary_openshift_api_server_url} with your credentials username: {secondary_openshift_cluster_admin_username} and password: {secondary_openshift_cluster_admin_password}.
+
[source,role="execute"]
----
oc login -u %secondary_openshift_cluster_admin_username% -p %secondary_openshift_cluster_admin_password% %secondary_openshift_api_server_url%
----
+
. You will now need to create the *globex* namespace in the secondary cluster and annotate it with the globex subscription name.
+
[source,role="execute"]
----
oc new-project globex || oc project globex
oc annotate ns globex --overwrite=true apps.open-cluster-management.io/hosting-subscription=globex/globex-subscription-1
----
+
. Switch back to the RHACM console and select *Applications* and filter on *Subscription*.  Click the 3 dots at the end of the *globex* application and select *Failover application*.
+
image:images/rhacm-failover-menu.png[images/rhacm-failover-menu.png]
+
. Provide the following details on the popup:
.. *Select policy*: drsync5m
.. *Target cluster*: secondary
.. *Select subscriptions group*: globex-subscription-1
+
image:images/rhacm-failover-application.png[images/rhacm-failover-application.png]
+
NOTE: The subscription group may not be in a ready state to select.  Wait a few minutes and try again.
.. Click *Initiate*
. You will notice that the pods on the primary cluster in the *globex* namespace will start terminating as the app starts failing over. Also check that the PVC will be in terminating state as well. Ensure that PVC is terminated and not visible on the primary.
+
image:images/primary-pods-terminated.png[images/primary-pods-terminated.png]
+
Review the status of the pvc (terminating) and the cephblockpool on primary. The health may be temporary in warning state and then goes back to healthy state:
+
[source,role="execute"]
----
oc login -u %primary_openshift_cluster_admin_username% -p %primary_openshift_cluster_admin_password% %primary_openshift_api_server_url%
----
+
[source,role="execute"]
----
oc get cephblockpool ocs-storagecluster-cephblockpool -n openshift-storage -o jsonpath='{.status.mirroringStatus.summary}{"\n"}'
----
+
.Example output.
----
map[daemon_health:OK health:OK image_health:OK states:map[replaying:2]]
----
+
. Check the pvc's (persistent volumen claims) being terminated and eventually disappears.
+
[source,role="execute"]
----
oc get pvc -n globex
----
+
.Example output.
----
NAME                 STATUS        VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS                  AGE
catalog-database     Terminating   pvc-faae1a1e-4476-4052-9461-bb7e495b5903   5Gi        RWO            ocs-storagecluster-ceph-rbd   45m
inventory-database   Terminating   pvc-fda1a2a3-cb5a-495d-b9c0-47ab95bba2c7   5Gi        RWO            ocs-storagecluster-ceph-rbd   45m
----
+
. Open another browser window and login to the *secondary* cluster link:{secondary_openshift_cluster_console_url}[Secondary Cluster OpenShift console] and log in with your credentials username: {secondary_openshift_cluster_admin_username} and password: {secondary_openshift_cluster_admin_password}.
+
image:images/openshift-login.png[images/openshift-login.png]
+
. Once logged in click *Workloads->Pods* on the left menu.  Ensure your selected project is *globex*.  Notice that the application pods have been recreated in this namespace.
+
image:images/secondary-pods-created.png[images/secondary-pods-created.png]
+
. Next, click *Networking->Routes* on the left menu and then click on the *Location* link of the *globex-ui* route.
+
image:images/secondary-globex-route.png[images/secondary-globex-route.png]
+
. Once you've clicked the link you will be redirected to globex online store front end.  Select *Cool Stuff Store* at the top of the page.
+
image:images/primary-globex-landing.png[images/primary-globex-landing.png]
+
. You can also get route from terminal window which you see on the right side of your instruction guide, login to the secondary cluster api link:{secondary_openshift_cluster_console_url}[Secondary Cluster OpenShift Cluster API link] with your credentials username: {secondary_openshift_cluster_admin_username} and password: {secondary_openshift_cluster_admin_password}.
+
[source,role="execute"]
----
oc login -u %secondary_openshift_cluster_admin_username% -p %secondary_openshift_cluster_admin_password% %secondary_openshift_api_server_url%
----
+
. You are now going to get the route of globex-ui and copy the route for example *globex-ui-globex.apps.cluster-flxz7-2.sandbox2418.opentlc.com* and paste in a browser window to get the coolstore app UI. This is the same route you can see from previous step by login to OpenShift Console:
+
[source,role="execute"]
----
oc get route -n globex | grep globex-ui
----
+
.Example output.
----
globex-ui                     globex-ui-globex.apps.cluster-flxz7-2.sandbox2418.opentlc.com
  globex-ui                     http   edge/Redirect   None
----
+
. On the Coolstore Globex UI, select the last page (6) and notice that the price of the Quarkust T-shirt is *$15.00*.  Our data from the primary cluster has been preserved during the failover.
+
image:images/secondary-price-preserved.png[images/secondary-price-preserved.png]
+
Review the status of the pvc and the cephblockpool on secondary. The health may be temporary in warning state and then goes back to healthy state. You may also see `states:map[replaying:2]`:
+
[source,role="execute"]
----
oc login -u %secondary_openshift_cluster_admin_username% -p %secondary_openshift_cluster_admin_password% %secondary_openshift_api_server_url%
----
+
[source,role="execute"]
----
oc get cephblockpool ocs-storagecluster-cephblockpool -n openshift-storage -o jsonpath='{.status.mirroringStatus.summary}{"\n"}'
----
+
.Example output.
----
map[daemon_health:OK health:OK image_health:OK states:map[replaying:2]]
----
+
[source,role="execute"]
----
oc get pvc -n globex
----
+
.Example output.
----
NAME                 STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS                  AGE
catalog-database     Bound    pvc-faae1a1e-4476-4052-9461-bb7e495b5903   5Gi        RWO            ocs-storagecluster-ceph-rbd   15m
inventory-database   Bound    pvc-fda1a2a3-cb5a-495d-b9c0-47ab95bba2c7   5Gi        RWO            ocs-storagecluster-ceph-rbd   15m
----

[NOTE] 
For production environment or real life scenario, the applications routes will be updated in Global Load Balancer using automated way for e.g. using ansible to update GLB's with the new updated routes post failover / relocate.